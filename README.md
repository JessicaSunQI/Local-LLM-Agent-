The process is to use Ollama LLM Inference Model 
It can be integrated into local development tool like vscode and server like your local agent 
it can read your code, optimize it, and generate the code 
